{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Swati', 'emotion'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I split the dataset into a train and test ds\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"michsethowusu/swati-emotions-corpus\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "   dataset[split] = dataset[split].shuffle(seed=42).select(range(15000))\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Swati', 'label'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Swati', 'label'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Swati': ['Loko kutocinisekisa kuphepha kwakho njengafakazi, nekubakhona kwebufakazi, nekuvimbela kutsi uhocise ekwetfuleni bufakazi ngenca yalokufakwa umoya.',\n",
       "  'Bantfu bafundzani ngaloku lokwenteka kulendzatjana?'],\n",
       " 'label': ['neutral', 'surprise']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i need to remove some columns not required for this task\n",
    "#dataset = dataset.remove_columns(['label', 'dataType', 'communityName', 'datetime'])\n",
    "\n",
    "# Rename 'emotion' to 'label' for easier PyTorch handling\n",
    "dataset = dataset.rename_column('emotion', 'label')\n",
    "\n",
    "# Inspect the dataset\n",
    "print(dataset)\n",
    "\n",
    "dataset[\"train\"][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf89431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling lables that are strings\n",
    "\n",
    "unique_labels = list(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "# Step 2: Create label2id and id2label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c7d579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Swati', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    encodings = tokenizer(batch[\"Swati\"], padding=\"max_length\", truncation=True)\n",
    "    encodings[\"label\"] = [label2id[label] for label in batch[\"label\"]]\n",
    "    return encodings\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess_batch, batched=True)\n",
    "    \n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36891a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=7\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61fef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"/workspace/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_latest.pth\")\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Keep only the last 3 checkpoints\n",
    "checkpoints = sorted(os.listdir(checkpoint_dir), reverse=True)\n",
    "if len(checkpoints) > 3:\n",
    "    os.remove(os.path.join(checkpoint_dir, checkpoints[-1]))  # Delete the oldest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fde3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff12160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:48:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9242671728134155,\n",
       " 'eval_accuracy': 0.32893333333333336,\n",
       " 'eval_runtime': 240.4599,\n",
       " 'eval_samples_per_second': 62.38,\n",
       " 'eval_steps_per_second': 7.798}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,413,134 || all params: 67,776,014 || trainable%: 2.08500606128888\n"
     ]
    }
   ],
   "source": [
    "#ref:https://huggingface.co/docs/peft/task_guides/lora_based_methods\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\n",
    "    #target_modules = [\"score\", \"c_attn\", \"c_proj\"], #not found in the base model\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "perf_model = get_peft_model(model, peft_config)\n",
    "perf_model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ede217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4690' max='4690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4690/4690 1:07:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.433600</td>\n",
       "      <td>1.380911</td>\n",
       "      <td>0.599133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.368500</td>\n",
       "      <td>1.379657</td>\n",
       "      <td>0.599133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.384100</td>\n",
       "      <td>1.381476</td>\n",
       "      <td>0.599133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.380600</td>\n",
       "      <td>1.380786</td>\n",
       "      <td>0.599133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.383600</td>\n",
       "      <td>1.380623</td>\n",
       "      <td>0.599133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/peft-distilbert/checkpoint-938 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft-distilbert/checkpoint-1876 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft-distilbert/checkpoint-2814 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft-distilbert/checkpoint-3752 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft-distilbert/checkpoint-4690 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4690, training_loss=1.3899295050452258, metrics={'train_runtime': 4079.2157, 'train_samples_per_second': 18.386, 'train_steps_per_second': 1.15, 'total_flos': 1.01242142208e+16, 'train_loss': 1.3899295050452258, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_trainer = Trainer(\n",
    "    model=perf_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/peft-distilbert\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "fine_tuned_trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft_model/tokenizer_config.json',\n",
       " './peft_model/special_tokens_map.json',\n",
       " './peft_model/vocab.txt',\n",
       " './peft_model/added_tokens.json',\n",
       " './peft_model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model the fine-tuned PEFT model\n",
    "from peft import PeftModel\n",
    "\n",
    "save_path = \"./peft_model\"\n",
    "perf_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load base model again\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=7)\n",
    "\n",
    "# Reloading the model\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"peft_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: {'eval_loss': 1.37965726852417, 'eval_accuracy': 0.5991333333333333, 'eval_runtime': 255.865, 'eval_samples_per_second': 58.625, 'eval_steps_per_second': 7.328}\n",
      "\n",
      "Fine-Tuned Model: {'eval_loss': 1.37965726852417, 'eval_accuracy': 0.5991333333333333, 'eval_runtime': 261.7086, 'eval_samples_per_second': 57.316, 'eval_steps_per_second': 3.584, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "#compare the original and fine-tuned models\n",
    "original_performance = trainer.evaluate()\n",
    "fine_tuned_performance = fine_tuned_trainer.evaluate()\n",
    "\n",
    "print(\"Original Model:\", original_performance)\n",
    "print()\n",
    "print(\"Fine-Tuned Model:\", fine_tuned_performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67819a",
   "metadata": {},
   "source": [
    "#### Using the same evaluation process (same metric(s) and dataset) we see a difference between the fine-tuned version and the original version of the model. The eval_accuracy': 0.3289336 earlier then later 0.59991 after fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed6d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
